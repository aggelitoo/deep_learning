{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotnine import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"covtype.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aujo8\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotnine\\stats\\stat_bin.py:109: PlotnineWarning: 'stat_bin()' using 'bins = 251'. Pick better value with 'binwidth'.\n"
     ]
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {
      "image/png": {
       "height": 480,
       "width": 640
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ggplot(df, aes(x=\"Cover_Type\")) +\\\n",
    "    geom_histogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    m = np.mean(data, axis=0)\n",
    "    s = np.std(data, axis=0)\n",
    "    strd_data = (data - m)/s\n",
    "    return strd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, testval = train_test_split(df, test_size=0.2, random_state=1995)\n",
    "test, val = train_test_split(testval, test_size=0.5, random_state=1995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing the non-categorical features (the first ten features)\n",
    "# also seperating the data into targets y and input features X\n",
    "X_train = pd.concat([standardize(train.iloc[:, 0:10]), train.iloc[:, 10:-1]], axis=1)\n",
    "y_train = train[\"Cover_Type\"]\n",
    "# y_train = pd.get_dummies(train[\"Cover_Type\"], dtype=int)\n",
    "\n",
    "X_test = pd.concat([standardize(test.iloc[:, 0:10]), test.iloc[:, 10:-1]], axis=1)\n",
    "y_test = test[\"Cover_Type\"]\n",
    "# y_test = pd.get_dummies(test[\"Cover_Type\"], dtype=int)\n",
    "\n",
    "X_val = pd.concat([standardize(val.iloc[:, 0:10]), val.iloc[:, 10:-1]], axis=1)\n",
    "y_val = val[\"Cover_Type\"]\n",
    "# y_val = pd.get_dummies(val[\"Cover_Type\"], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# if possible, running on GPU\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming our data into tensors\n",
    "X_train_tensor = torch.tensor(X_train.values).float()\n",
    "y_train_tensor = torch.tensor(y_train.values).float()\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values).float() \n",
    "y_test_tensor = torch.tensor(y_test.values).float()\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val.values).float()\n",
    "y_val_tensor = torch.tensor(y_val.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating torch datasets in preparation for mini batches\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_ds = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "val_ds = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# the dataloaders for the mini batches\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(X_train_tensor.shape[1], 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 70),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(70, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 7)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=54, out_features=60, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=60, out_features=70, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=70, out_features=60, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=60, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiating the model\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.094826  [   64/464809]\n",
      "loss: 0.939991  [ 6464/464809]\n",
      "loss: 1.034701  [12864/464809]\n",
      "loss: 1.078665  [19264/464809]\n",
      "loss: 0.984511  [25664/464809]\n",
      "loss: 0.969837  [32064/464809]\n",
      "loss: 1.099041  [38464/464809]\n",
      "loss: 0.984653  [44864/464809]\n",
      "loss: 1.000488  [51264/464809]\n",
      "loss: 1.045063  [57664/464809]\n",
      "loss: 1.069505  [64064/464809]\n",
      "loss: 1.105978  [70464/464809]\n",
      "loss: 0.820098  [76864/464809]\n",
      "loss: 1.187394  [83264/464809]\n",
      "loss: 1.021109  [89664/464809]\n",
      "loss: 0.893741  [96064/464809]\n",
      "loss: 0.891846  [102464/464809]\n",
      "loss: 0.924527  [108864/464809]\n",
      "loss: 0.935126  [115264/464809]\n",
      "loss: 0.840895  [121664/464809]\n",
      "loss: 1.020008  [128064/464809]\n",
      "loss: 0.906472  [134464/464809]\n",
      "loss: 1.112053  [140864/464809]\n",
      "loss: 0.882141  [147264/464809]\n",
      "loss: 0.931620  [153664/464809]\n",
      "loss: 0.872925  [160064/464809]\n",
      "loss: 0.874560  [166464/464809]\n",
      "loss: 0.892118  [172864/464809]\n",
      "loss: 0.857107  [179264/464809]\n",
      "loss: 1.101958  [185664/464809]\n",
      "loss: 1.073103  [192064/464809]\n",
      "loss: 1.057301  [198464/464809]\n",
      "loss: 0.861865  [204864/464809]\n",
      "loss: 0.994749  [211264/464809]\n",
      "loss: 1.008032  [217664/464809]\n",
      "loss: 1.018251  [224064/464809]\n",
      "loss: 0.928485  [230464/464809]\n",
      "loss: 1.151155  [236864/464809]\n",
      "loss: 0.882268  [243264/464809]\n",
      "loss: 0.910089  [249664/464809]\n",
      "loss: 0.860231  [256064/464809]\n",
      "loss: 1.063017  [262464/464809]\n",
      "loss: 1.017309  [268864/464809]\n",
      "loss: 0.850694  [275264/464809]\n",
      "loss: 1.023420  [281664/464809]\n",
      "loss: 0.873483  [288064/464809]\n",
      "loss: 0.921065  [294464/464809]\n",
      "loss: 0.924892  [300864/464809]\n",
      "loss: 0.681748  [307264/464809]\n",
      "loss: 0.812541  [313664/464809]\n",
      "loss: 0.996071  [320064/464809]\n",
      "loss: 1.027930  [326464/464809]\n",
      "loss: 0.942930  [332864/464809]\n",
      "loss: 0.902494  [339264/464809]\n",
      "loss: 0.726351  [345664/464809]\n",
      "loss: 1.021102  [352064/464809]\n",
      "loss: 0.805725  [358464/464809]\n",
      "loss: 0.820046  [364864/464809]\n",
      "loss: 0.882981  [371264/464809]\n",
      "loss: 1.015033  [377664/464809]\n",
      "loss: 0.973601  [384064/464809]\n",
      "loss: 0.942603  [390464/464809]\n",
      "loss: 0.740704  [396864/464809]\n",
      "loss: 0.784423  [403264/464809]\n",
      "loss: 0.899818  [409664/464809]\n",
      "loss: 0.838671  [416064/464809]\n",
      "loss: 0.807762  [422464/464809]\n",
      "loss: 0.822365  [428864/464809]\n",
      "loss: 0.882058  [435264/464809]\n",
      "loss: 0.789342  [441664/464809]\n",
      "loss: 0.956438  [448064/464809]\n",
      "loss: 0.784511  [454464/464809]\n",
      "loss: 0.895266  [460864/464809]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.842461  [   64/464809]\n",
      "loss: 0.854821  [ 6464/464809]\n",
      "loss: 0.920217  [12864/464809]\n",
      "loss: 0.832837  [19264/464809]\n",
      "loss: 0.767031  [25664/464809]\n",
      "loss: 0.755323  [32064/464809]\n",
      "loss: 0.758267  [38464/464809]\n",
      "loss: 0.889025  [44864/464809]\n",
      "loss: 0.748339  [51264/464809]\n",
      "loss: 0.683712  [57664/464809]\n",
      "loss: 0.933617  [64064/464809]\n",
      "loss: 0.732642  [70464/464809]\n",
      "loss: 0.931957  [76864/464809]\n",
      "loss: 0.908929  [83264/464809]\n",
      "loss: 0.781914  [89664/464809]\n",
      "loss: 0.706050  [96064/464809]\n",
      "loss: 0.732670  [102464/464809]\n",
      "loss: 0.766066  [108864/464809]\n",
      "loss: 0.760953  [115264/464809]\n",
      "loss: 0.792659  [121664/464809]\n",
      "loss: 0.870183  [128064/464809]\n",
      "loss: 0.825515  [134464/464809]\n",
      "loss: 0.862584  [140864/464809]\n",
      "loss: 0.925926  [147264/464809]\n",
      "loss: 0.861602  [153664/464809]\n",
      "loss: 0.825107  [160064/464809]\n",
      "loss: 0.637037  [166464/464809]\n",
      "loss: 0.742659  [172864/464809]\n",
      "loss: 0.894373  [179264/464809]\n",
      "loss: 0.907847  [185664/464809]\n",
      "loss: 1.032298  [192064/464809]\n",
      "loss: 0.845076  [198464/464809]\n",
      "loss: 0.602901  [204864/464809]\n",
      "loss: 0.696798  [211264/464809]\n",
      "loss: 0.603793  [217664/464809]\n",
      "loss: 0.713901  [224064/464809]\n",
      "loss: 0.818269  [230464/464809]\n",
      "loss: 0.708388  [236864/464809]\n",
      "loss: 0.695031  [243264/464809]\n",
      "loss: 0.812288  [249664/464809]\n",
      "loss: 0.661319  [256064/464809]\n",
      "loss: 0.732270  [262464/464809]\n",
      "loss: 0.868508  [268864/464809]\n",
      "loss: 0.810574  [275264/464809]\n",
      "loss: 0.926124  [281664/464809]\n",
      "loss: 0.785731  [288064/464809]\n",
      "loss: 0.623725  [294464/464809]\n",
      "loss: 0.718822  [300864/464809]\n",
      "loss: 0.800328  [307264/464809]\n",
      "loss: 0.975390  [313664/464809]\n",
      "loss: 0.790286  [320064/464809]\n",
      "loss: 0.865602  [326464/464809]\n",
      "loss: 0.674862  [332864/464809]\n",
      "loss: 0.806777  [339264/464809]\n",
      "loss: 0.690342  [345664/464809]\n",
      "loss: 0.790487  [352064/464809]\n",
      "loss: 0.896845  [358464/464809]\n",
      "loss: 0.595887  [364864/464809]\n",
      "loss: 0.599575  [371264/464809]\n",
      "loss: 0.800377  [377664/464809]\n",
      "loss: 0.964700  [384064/464809]\n",
      "loss: 0.849910  [390464/464809]\n",
      "loss: 0.602436  [396864/464809]\n",
      "loss: 0.792629  [403264/464809]\n",
      "loss: 0.815130  [409664/464809]\n",
      "loss: 0.641099  [416064/464809]\n",
      "loss: 0.815882  [422464/464809]\n",
      "loss: 0.757133  [428864/464809]\n",
      "loss: 0.909467  [435264/464809]\n",
      "loss: 0.796654  [441664/464809]\n",
      "loss: 0.678201  [448064/464809]\n",
      "loss: 0.679734  [454464/464809]\n",
      "loss: 0.594123  [460864/464809]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.991959  [   64/464809]\n",
      "loss: 0.784221  [ 6464/464809]\n",
      "loss: 0.875773  [12864/464809]\n",
      "loss: 0.817003  [19264/464809]\n",
      "loss: 0.783553  [25664/464809]\n",
      "loss: 0.760153  [32064/464809]\n",
      "loss: 0.877189  [38464/464809]\n",
      "loss: 0.922379  [44864/464809]\n",
      "loss: 0.702196  [51264/464809]\n",
      "loss: 0.753672  [57664/464809]\n",
      "loss: 0.772007  [64064/464809]\n",
      "loss: 0.732733  [70464/464809]\n",
      "loss: 0.884144  [76864/464809]\n",
      "loss: 0.643955  [83264/464809]\n",
      "loss: 0.648838  [89664/464809]\n",
      "loss: 0.848870  [96064/464809]\n",
      "loss: 0.739939  [102464/464809]\n",
      "loss: 0.720928  [108864/464809]\n",
      "loss: 0.755034  [115264/464809]\n",
      "loss: 0.770622  [121664/464809]\n",
      "loss: 0.738732  [128064/464809]\n",
      "loss: 0.616389  [134464/464809]\n",
      "loss: 0.748951  [140864/464809]\n",
      "loss: 0.723199  [147264/464809]\n",
      "loss: 0.755270  [153664/464809]\n",
      "loss: 0.572474  [160064/464809]\n",
      "loss: 0.716152  [166464/464809]\n",
      "loss: 0.617706  [172864/464809]\n",
      "loss: 0.949834  [179264/464809]\n",
      "loss: 0.659649  [185664/464809]\n",
      "loss: 0.674074  [192064/464809]\n",
      "loss: 0.815487  [198464/464809]\n",
      "loss: 0.799061  [204864/464809]\n",
      "loss: 0.743474  [211264/464809]\n",
      "loss: 0.736793  [217664/464809]\n",
      "loss: 0.816861  [224064/464809]\n",
      "loss: 0.777265  [230464/464809]\n",
      "loss: 0.640576  [236864/464809]\n",
      "loss: 0.898374  [243264/464809]\n",
      "loss: 0.700026  [249664/464809]\n",
      "loss: 0.713813  [256064/464809]\n",
      "loss: 0.807009  [262464/464809]\n",
      "loss: 0.734323  [268864/464809]\n",
      "loss: 0.729581  [275264/464809]\n",
      "loss: 0.773366  [281664/464809]\n",
      "loss: 0.668751  [288064/464809]\n",
      "loss: 0.753060  [294464/464809]\n",
      "loss: 0.683361  [300864/464809]\n",
      "loss: 0.621542  [307264/464809]\n",
      "loss: 0.728167  [313664/464809]\n",
      "loss: 0.796113  [320064/464809]\n",
      "loss: 0.722893  [326464/464809]\n",
      "loss: 0.795410  [332864/464809]\n",
      "loss: 0.676308  [339264/464809]\n",
      "loss: 0.625523  [345664/464809]\n",
      "loss: 0.706137  [352064/464809]\n",
      "loss: 0.838410  [358464/464809]\n",
      "loss: 0.830343  [364864/464809]\n",
      "loss: 0.841925  [371264/464809]\n",
      "loss: 0.605626  [377664/464809]\n",
      "loss: 0.776622  [384064/464809]\n",
      "loss: 0.750125  [390464/464809]\n",
      "loss: 0.718851  [396864/464809]\n",
      "loss: 0.733891  [403264/464809]\n",
      "loss: 0.696089  [409664/464809]\n",
      "loss: 0.631675  [416064/464809]\n",
      "loss: 0.967687  [422464/464809]\n",
      "loss: 0.812411  [428864/464809]\n",
      "loss: 0.811702  [435264/464809]\n",
      "loss: 0.705299  [441664/464809]\n",
      "loss: 0.697850  [448064/464809]\n",
      "loss: 0.610464  [454464/464809]\n",
      "loss: 0.660849  [460864/464809]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.726051  [   64/464809]\n",
      "loss: 0.792306  [ 6464/464809]\n",
      "loss: 0.918696  [12864/464809]\n",
      "loss: 0.833694  [19264/464809]\n",
      "loss: 0.741863  [25664/464809]\n",
      "loss: 0.767687  [32064/464809]\n",
      "loss: 0.794155  [38464/464809]\n",
      "loss: 0.618268  [44864/464809]\n",
      "loss: 0.798556  [51264/464809]\n",
      "loss: 0.679068  [57664/464809]\n",
      "loss: 0.742053  [64064/464809]\n",
      "loss: 0.795519  [70464/464809]\n",
      "loss: 0.650717  [76864/464809]\n",
      "loss: 0.668485  [83264/464809]\n",
      "loss: 0.676894  [89664/464809]\n",
      "loss: 0.614768  [96064/464809]\n",
      "loss: 0.677477  [102464/464809]\n",
      "loss: 0.822606  [108864/464809]\n",
      "loss: 0.580305  [115264/464809]\n",
      "loss: 0.792359  [121664/464809]\n",
      "loss: 0.892956  [128064/464809]\n",
      "loss: 0.774392  [134464/464809]\n",
      "loss: 0.773031  [140864/464809]\n",
      "loss: 0.862810  [147264/464809]\n",
      "loss: 0.614708  [153664/464809]\n",
      "loss: 0.733333  [160064/464809]\n",
      "loss: 0.697770  [166464/464809]\n",
      "loss: 0.897066  [172864/464809]\n",
      "loss: 0.703544  [179264/464809]\n",
      "loss: 0.654757  [185664/464809]\n",
      "loss: 0.876705  [192064/464809]\n",
      "loss: 0.738927  [198464/464809]\n",
      "loss: 0.627183  [204864/464809]\n",
      "loss: 0.850945  [211264/464809]\n",
      "loss: 0.805695  [217664/464809]\n",
      "loss: 0.523215  [224064/464809]\n",
      "loss: 0.769732  [230464/464809]\n",
      "loss: 0.711495  [236864/464809]\n",
      "loss: 0.696503  [243264/464809]\n",
      "loss: 0.532658  [249664/464809]\n",
      "loss: 0.831986  [256064/464809]\n",
      "loss: 0.713012  [262464/464809]\n",
      "loss: 0.663674  [268864/464809]\n",
      "loss: 0.771339  [275264/464809]\n",
      "loss: 0.621452  [281664/464809]\n",
      "loss: 0.722611  [288064/464809]\n",
      "loss: 0.769015  [294464/464809]\n",
      "loss: 0.778824  [300864/464809]\n",
      "loss: 0.669311  [307264/464809]\n",
      "loss: 0.653351  [313664/464809]\n",
      "loss: 0.780994  [320064/464809]\n",
      "loss: 0.645172  [326464/464809]\n",
      "loss: 0.898110  [332864/464809]\n",
      "loss: 0.594613  [339264/464809]\n",
      "loss: 0.730202  [345664/464809]\n",
      "loss: 0.753797  [352064/464809]\n",
      "loss: 0.702426  [358464/464809]\n",
      "loss: 0.848436  [364864/464809]\n",
      "loss: 0.710290  [371264/464809]\n",
      "loss: 0.669706  [377664/464809]\n",
      "loss: 0.784425  [384064/464809]\n",
      "loss: 0.665851  [390464/464809]\n",
      "loss: 0.605586  [396864/464809]\n",
      "loss: 0.669533  [403264/464809]\n",
      "loss: 0.712522  [409664/464809]\n",
      "loss: 0.814348  [416064/464809]\n",
      "loss: 0.754675  [422464/464809]\n",
      "loss: 0.756237  [428864/464809]\n",
      "loss: 0.648774  [435264/464809]\n",
      "loss: 0.559930  [441664/464809]\n",
      "loss: 1.080553  [448064/464809]\n",
      "loss: 0.743975  [454464/464809]\n",
      "loss: 0.674109  [460864/464809]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.703658  [   64/464809]\n",
      "loss: 0.602340  [ 6464/464809]\n",
      "loss: 0.699077  [12864/464809]\n",
      "loss: 0.766420  [19264/464809]\n",
      "loss: 0.712903  [25664/464809]\n",
      "loss: 0.618790  [32064/464809]\n",
      "loss: 0.732050  [38464/464809]\n",
      "loss: 0.605275  [44864/464809]\n",
      "loss: 0.646589  [51264/464809]\n",
      "loss: 0.751413  [57664/464809]\n",
      "loss: 0.649083  [64064/464809]\n",
      "loss: 0.654745  [70464/464809]\n",
      "loss: 0.702329  [76864/464809]\n",
      "loss: 0.686954  [83264/464809]\n",
      "loss: 0.875787  [89664/464809]\n",
      "loss: 0.734387  [96064/464809]\n",
      "loss: 0.623290  [102464/464809]\n",
      "loss: 0.695297  [108864/464809]\n",
      "loss: 0.853216  [115264/464809]\n",
      "loss: 0.702048  [121664/464809]\n",
      "loss: 0.832037  [128064/464809]\n",
      "loss: 0.553225  [134464/464809]\n",
      "loss: 0.757272  [140864/464809]\n",
      "loss: 0.720868  [147264/464809]\n",
      "loss: 0.658629  [153664/464809]\n",
      "loss: 0.739589  [160064/464809]\n",
      "loss: 0.675946  [166464/464809]\n",
      "loss: 0.756489  [172864/464809]\n",
      "loss: 0.800735  [179264/464809]\n",
      "loss: 0.854674  [185664/464809]\n",
      "loss: 0.877253  [192064/464809]\n",
      "loss: 0.732860  [198464/464809]\n",
      "loss: 0.804027  [204864/464809]\n",
      "loss: 0.618448  [211264/464809]\n",
      "loss: 0.949337  [217664/464809]\n",
      "loss: 0.612909  [224064/464809]\n",
      "loss: 0.805081  [230464/464809]\n",
      "loss: 0.810521  [236864/464809]\n",
      "loss: 0.749173  [243264/464809]\n",
      "loss: 0.597757  [249664/464809]\n",
      "loss: 0.936372  [256064/464809]\n",
      "loss: 0.531463  [262464/464809]\n",
      "loss: 0.640222  [268864/464809]\n",
      "loss: 0.719027  [275264/464809]\n",
      "loss: 0.645152  [281664/464809]\n",
      "loss: 0.635568  [288064/464809]\n",
      "loss: 0.768544  [294464/464809]\n",
      "loss: 0.566248  [300864/464809]\n",
      "loss: 0.543872  [307264/464809]\n",
      "loss: 0.534910  [313664/464809]\n",
      "loss: 0.726434  [320064/464809]\n",
      "loss: 0.643017  [326464/464809]\n",
      "loss: 0.776659  [332864/464809]\n",
      "loss: 0.830767  [339264/464809]\n",
      "loss: 0.699093  [345664/464809]\n",
      "loss: 0.545446  [352064/464809]\n",
      "loss: 0.608175  [358464/464809]\n",
      "loss: 0.582016  [364864/464809]\n",
      "loss: 0.596613  [371264/464809]\n",
      "loss: 0.636338  [377664/464809]\n",
      "loss: 0.558583  [384064/464809]\n",
      "loss: 0.596405  [390464/464809]\n",
      "loss: 0.698547  [396864/464809]\n",
      "loss: 0.613589  [403264/464809]\n",
      "loss: 0.588047  [409664/464809]\n",
      "loss: 0.730771  [416064/464809]\n",
      "loss: 0.513205  [422464/464809]\n",
      "loss: 0.554595  [428864/464809]\n",
      "loss: 0.521718  [435264/464809]\n",
      "loss: 0.680051  [441664/464809]\n",
      "loss: 0.578451  [448064/464809]\n",
      "loss: 0.686356  [454464/464809]\n",
      "loss: 0.702371  [460864/464809]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.606768  [   64/464809]\n",
      "loss: 0.749797  [ 6464/464809]\n",
      "loss: 0.710411  [12864/464809]\n",
      "loss: 0.665771  [19264/464809]\n",
      "loss: 0.543209  [25664/464809]\n",
      "loss: 0.768326  [32064/464809]\n",
      "loss: 0.696648  [38464/464809]\n",
      "loss: 0.595706  [44864/464809]\n",
      "loss: 0.782170  [51264/464809]\n",
      "loss: 0.530699  [57664/464809]\n",
      "loss: 0.613672  [64064/464809]\n",
      "loss: 0.673788  [70464/464809]\n",
      "loss: 0.584528  [76864/464809]\n",
      "loss: 0.495528  [83264/464809]\n",
      "loss: 0.751865  [89664/464809]\n",
      "loss: 0.531606  [96064/464809]\n",
      "loss: 0.831989  [102464/464809]\n",
      "loss: 0.754614  [108864/464809]\n",
      "loss: 0.761380  [115264/464809]\n",
      "loss: 0.578968  [121664/464809]\n",
      "loss: 0.605875  [128064/464809]\n",
      "loss: 0.685750  [134464/464809]\n",
      "loss: 0.778984  [140864/464809]\n",
      "loss: 0.612869  [147264/464809]\n",
      "loss: 0.631455  [153664/464809]\n",
      "loss: 0.765214  [160064/464809]\n",
      "loss: 0.639968  [166464/464809]\n",
      "loss: 0.738116  [172864/464809]\n",
      "loss: 0.628751  [179264/464809]\n",
      "loss: 0.535494  [185664/464809]\n",
      "loss: 0.697991  [192064/464809]\n",
      "loss: 0.613264  [198464/464809]\n",
      "loss: 0.723092  [204864/464809]\n",
      "loss: 0.655349  [211264/464809]\n",
      "loss: 0.651375  [217664/464809]\n",
      "loss: 0.780702  [224064/464809]\n",
      "loss: 0.760071  [230464/464809]\n",
      "loss: 0.662627  [236864/464809]\n",
      "loss: 0.642819  [243264/464809]\n",
      "loss: 0.547099  [249664/464809]\n",
      "loss: 0.749593  [256064/464809]\n",
      "loss: 0.663850  [262464/464809]\n",
      "loss: 0.624348  [268864/464809]\n",
      "loss: 0.673310  [275264/464809]\n",
      "loss: 0.691208  [281664/464809]\n",
      "loss: 0.718215  [288064/464809]\n",
      "loss: 0.648299  [294464/464809]\n",
      "loss: 0.631955  [300864/464809]\n",
      "loss: 0.740216  [307264/464809]\n",
      "loss: 0.671561  [313664/464809]\n",
      "loss: 0.757751  [320064/464809]\n",
      "loss: 0.645334  [326464/464809]\n",
      "loss: 0.649677  [332864/464809]\n",
      "loss: 0.676757  [339264/464809]\n",
      "loss: 0.721526  [345664/464809]\n",
      "loss: 0.497937  [352064/464809]\n",
      "loss: 0.840045  [358464/464809]\n",
      "loss: 0.761701  [364864/464809]\n",
      "loss: 0.672032  [371264/464809]\n",
      "loss: 0.652337  [377664/464809]\n",
      "loss: 0.602816  [384064/464809]\n",
      "loss: 0.626094  [390464/464809]\n",
      "loss: 0.662250  [396864/464809]\n",
      "loss: 0.689588  [403264/464809]\n",
      "loss: 0.772052  [409664/464809]\n",
      "loss: 0.789488  [416064/464809]\n",
      "loss: 0.588798  [422464/464809]\n",
      "loss: 0.564670  [428864/464809]\n",
      "loss: 0.655365  [435264/464809]\n",
      "loss: 0.888702  [441664/464809]\n",
      "loss: 0.770485  [448064/464809]\n",
      "loss: 0.667241  [454464/464809]\n",
      "loss: 0.733124  [460864/464809]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.651804  [   64/464809]\n",
      "loss: 0.596928  [ 6464/464809]\n",
      "loss: 0.750291  [12864/464809]\n",
      "loss: 0.672184  [19264/464809]\n",
      "loss: 0.694928  [25664/464809]\n",
      "loss: 0.681472  [32064/464809]\n",
      "loss: 0.699717  [38464/464809]\n",
      "loss: 0.598800  [44864/464809]\n",
      "loss: 0.771233  [51264/464809]\n",
      "loss: 0.772427  [57664/464809]\n",
      "loss: 0.670309  [64064/464809]\n",
      "loss: 0.657962  [70464/464809]\n",
      "loss: 0.738209  [76864/464809]\n",
      "loss: 0.737425  [83264/464809]\n",
      "loss: 0.743403  [89664/464809]\n",
      "loss: 0.511962  [96064/464809]\n",
      "loss: 0.701482  [102464/464809]\n",
      "loss: 0.587974  [108864/464809]\n",
      "loss: 0.643840  [115264/464809]\n",
      "loss: 0.794788  [121664/464809]\n",
      "loss: 0.804958  [128064/464809]\n",
      "loss: 0.676669  [134464/464809]\n",
      "loss: 0.554268  [140864/464809]\n",
      "loss: 0.657291  [147264/464809]\n",
      "loss: 0.534988  [153664/464809]\n",
      "loss: 0.583663  [160064/464809]\n",
      "loss: 0.642921  [166464/464809]\n",
      "loss: 0.560166  [172864/464809]\n",
      "loss: 0.583109  [179264/464809]\n",
      "loss: 0.704635  [185664/464809]\n",
      "loss: 0.881849  [192064/464809]\n",
      "loss: 0.749976  [198464/464809]\n",
      "loss: 0.679417  [204864/464809]\n",
      "loss: 0.655082  [211264/464809]\n",
      "loss: 0.555770  [217664/464809]\n",
      "loss: 0.691706  [224064/464809]\n",
      "loss: 0.715892  [230464/464809]\n",
      "loss: 0.589618  [236864/464809]\n",
      "loss: 0.721586  [243264/464809]\n",
      "loss: 0.672019  [249664/464809]\n",
      "loss: 0.842873  [256064/464809]\n",
      "loss: 0.495686  [262464/464809]\n",
      "loss: 0.621167  [268864/464809]\n",
      "loss: 0.633509  [275264/464809]\n",
      "loss: 0.576658  [281664/464809]\n",
      "loss: 0.574114  [288064/464809]\n",
      "loss: 0.650997  [294464/464809]\n",
      "loss: 0.536467  [300864/464809]\n",
      "loss: 0.536298  [307264/464809]\n",
      "loss: 0.671268  [313664/464809]\n",
      "loss: 0.689677  [320064/464809]\n",
      "loss: 0.656160  [326464/464809]\n",
      "loss: 0.555063  [332864/464809]\n",
      "loss: 0.689371  [339264/464809]\n",
      "loss: 0.732028  [345664/464809]\n",
      "loss: 0.524242  [352064/464809]\n",
      "loss: 0.757876  [358464/464809]\n",
      "loss: 0.737846  [364864/464809]\n",
      "loss: 0.652421  [371264/464809]\n",
      "loss: 0.711686  [377664/464809]\n",
      "loss: 0.616698  [384064/464809]\n",
      "loss: 0.643474  [390464/464809]\n",
      "loss: 0.537680  [396864/464809]\n",
      "loss: 0.611634  [403264/464809]\n",
      "loss: 0.619904  [409664/464809]\n",
      "loss: 0.620214  [416064/464809]\n",
      "loss: 0.586971  [422464/464809]\n",
      "loss: 0.512481  [428864/464809]\n",
      "loss: 0.598273  [435264/464809]\n",
      "loss: 0.740227  [441664/464809]\n",
      "loss: 0.805370  [448064/464809]\n",
      "loss: 0.801046  [454464/464809]\n",
      "loss: 0.582232  [460864/464809]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.526091  [   64/464809]\n",
      "loss: 0.781291  [ 6464/464809]\n",
      "loss: 0.681360  [12864/464809]\n",
      "loss: 0.627047  [19264/464809]\n",
      "loss: 0.539771  [25664/464809]\n",
      "loss: 0.632599  [32064/464809]\n",
      "loss: 0.640662  [38464/464809]\n",
      "loss: 0.756095  [44864/464809]\n",
      "loss: 0.719982  [51264/464809]\n",
      "loss: 0.705376  [57664/464809]\n",
      "loss: 0.655360  [64064/464809]\n",
      "loss: 0.656050  [70464/464809]\n",
      "loss: 0.654663  [76864/464809]\n",
      "loss: 0.685385  [83264/464809]\n",
      "loss: 0.702744  [89664/464809]\n",
      "loss: 0.934249  [96064/464809]\n",
      "loss: 0.756853  [102464/464809]\n",
      "loss: 0.590483  [108864/464809]\n",
      "loss: 0.484439  [115264/464809]\n",
      "loss: 0.788629  [121664/464809]\n",
      "loss: 0.656170  [128064/464809]\n",
      "loss: 0.594415  [134464/464809]\n",
      "loss: 0.820313  [140864/464809]\n",
      "loss: 0.676114  [147264/464809]\n",
      "loss: 0.749291  [153664/464809]\n",
      "loss: 0.519111  [160064/464809]\n",
      "loss: 0.705871  [166464/464809]\n",
      "loss: 0.531186  [172864/464809]\n",
      "loss: 0.624480  [179264/464809]\n",
      "loss: 0.495370  [185664/464809]\n",
      "loss: 0.578077  [192064/464809]\n",
      "loss: 0.611448  [198464/464809]\n",
      "loss: 0.592131  [204864/464809]\n",
      "loss: 0.763047  [211264/464809]\n",
      "loss: 0.663741  [217664/464809]\n",
      "loss: 0.565219  [224064/464809]\n",
      "loss: 0.571715  [230464/464809]\n",
      "loss: 0.612465  [236864/464809]\n",
      "loss: 0.790509  [243264/464809]\n",
      "loss: 0.558325  [249664/464809]\n",
      "loss: 0.570064  [256064/464809]\n",
      "loss: 0.472360  [262464/464809]\n",
      "loss: 0.709554  [268864/464809]\n",
      "loss: 0.737523  [275264/464809]\n",
      "loss: 0.600255  [281664/464809]\n",
      "loss: 0.550540  [288064/464809]\n",
      "loss: 0.785131  [294464/464809]\n",
      "loss: 0.536240  [300864/464809]\n",
      "loss: 0.662347  [307264/464809]\n",
      "loss: 0.560858  [313664/464809]\n",
      "loss: 0.527615  [320064/464809]\n",
      "loss: 0.606088  [326464/464809]\n",
      "loss: 0.618946  [332864/464809]\n",
      "loss: 0.527110  [339264/464809]\n",
      "loss: 0.519927  [345664/464809]\n",
      "loss: 0.683837  [352064/464809]\n",
      "loss: 0.620775  [358464/464809]\n",
      "loss: 0.590043  [364864/464809]\n",
      "loss: 0.761399  [371264/464809]\n",
      "loss: 0.498774  [377664/464809]\n",
      "loss: 0.619320  [384064/464809]\n",
      "loss: 0.676661  [390464/464809]\n",
      "loss: 0.659711  [396864/464809]\n",
      "loss: 0.731056  [403264/464809]\n",
      "loss: 0.715339  [409664/464809]\n",
      "loss: 0.681316  [416064/464809]\n",
      "loss: 0.580437  [422464/464809]\n",
      "loss: 0.799724  [428864/464809]\n",
      "loss: 0.682956  [435264/464809]\n",
      "loss: 0.605927  [441664/464809]\n",
      "loss: 0.719635  [448064/464809]\n",
      "loss: 0.693169  [454464/464809]\n",
      "loss: 0.641964  [460864/464809]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.657330  [   64/464809]\n",
      "loss: 0.597373  [ 6464/464809]\n",
      "loss: 0.674740  [12864/464809]\n",
      "loss: 0.630086  [19264/464809]\n",
      "loss: 0.583318  [25664/464809]\n",
      "loss: 0.578979  [32064/464809]\n",
      "loss: 0.573161  [38464/464809]\n",
      "loss: 0.592942  [44864/464809]\n",
      "loss: 0.622443  [51264/464809]\n",
      "loss: 0.553696  [57664/464809]\n",
      "loss: 0.655092  [64064/464809]\n",
      "loss: 0.632950  [70464/464809]\n",
      "loss: 0.592307  [76864/464809]\n",
      "loss: 0.714256  [83264/464809]\n",
      "loss: 0.755710  [89664/464809]\n",
      "loss: 0.635629  [96064/464809]\n",
      "loss: 0.743658  [102464/464809]\n",
      "loss: 0.607349  [108864/464809]\n",
      "loss: 0.630183  [115264/464809]\n",
      "loss: 0.743242  [121664/464809]\n",
      "loss: 0.590762  [128064/464809]\n",
      "loss: 0.608655  [134464/464809]\n",
      "loss: 0.777306  [140864/464809]\n",
      "loss: 0.634470  [147264/464809]\n",
      "loss: 0.714887  [153664/464809]\n",
      "loss: 0.660686  [160064/464809]\n",
      "loss: 0.737905  [166464/464809]\n",
      "loss: 0.597680  [172864/464809]\n",
      "loss: 0.549870  [179264/464809]\n",
      "loss: 0.545139  [185664/464809]\n",
      "loss: 0.573755  [192064/464809]\n",
      "loss: 0.651658  [198464/464809]\n",
      "loss: 0.761427  [204864/464809]\n",
      "loss: 0.743055  [211264/464809]\n",
      "loss: 0.661298  [217664/464809]\n",
      "loss: 0.676633  [224064/464809]\n",
      "loss: 0.638358  [230464/464809]\n",
      "loss: 0.647456  [236864/464809]\n",
      "loss: 0.578714  [243264/464809]\n",
      "loss: 0.577830  [249664/464809]\n",
      "loss: 0.620965  [256064/464809]\n",
      "loss: 0.552077  [262464/464809]\n",
      "loss: 0.473769  [268864/464809]\n",
      "loss: 0.697778  [275264/464809]\n",
      "loss: 0.569862  [281664/464809]\n",
      "loss: 0.618590  [288064/464809]\n",
      "loss: 0.565050  [294464/464809]\n",
      "loss: 0.623536  [300864/464809]\n",
      "loss: 0.631609  [307264/464809]\n",
      "loss: 0.649402  [313664/464809]\n",
      "loss: 0.753309  [320064/464809]\n",
      "loss: 0.583601  [326464/464809]\n",
      "loss: 0.620593  [332864/464809]\n",
      "loss: 0.790524  [339264/464809]\n",
      "loss: 0.556261  [345664/464809]\n",
      "loss: 0.677289  [352064/464809]\n",
      "loss: 0.590863  [358464/464809]\n",
      "loss: 0.657522  [364864/464809]\n",
      "loss: 0.551301  [371264/464809]\n",
      "loss: 0.788124  [377664/464809]\n",
      "loss: 0.870358  [384064/464809]\n",
      "loss: 0.609214  [390464/464809]\n",
      "loss: 0.543257  [396864/464809]\n",
      "loss: 0.545314  [403264/464809]\n",
      "loss: 0.681716  [409664/464809]\n",
      "loss: 0.763654  [416064/464809]\n",
      "loss: 0.635950  [422464/464809]\n",
      "loss: 0.648937  [428864/464809]\n",
      "loss: 0.731954  [435264/464809]\n",
      "loss: 0.553891  [441664/464809]\n",
      "loss: 0.667617  [448064/464809]\n",
      "loss: 0.732598  [454464/464809]\n",
      "loss: 0.810667  [460864/464809]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.684121  [   64/464809]\n",
      "loss: 0.624402  [ 6464/464809]\n",
      "loss: 0.753301  [12864/464809]\n",
      "loss: 0.587846  [19264/464809]\n",
      "loss: 0.759473  [25664/464809]\n",
      "loss: 0.542948  [32064/464809]\n",
      "loss: 0.596377  [38464/464809]\n",
      "loss: 0.642419  [44864/464809]\n",
      "loss: 0.783588  [51264/464809]\n",
      "loss: 0.585598  [57664/464809]\n",
      "loss: 0.534865  [64064/464809]\n",
      "loss: 0.640685  [70464/464809]\n",
      "loss: 0.435364  [76864/464809]\n",
      "loss: 0.575487  [83264/464809]\n",
      "loss: 0.659789  [89664/464809]\n",
      "loss: 0.650868  [96064/464809]\n",
      "loss: 0.721878  [102464/464809]\n",
      "loss: 0.541332  [108864/464809]\n",
      "loss: 0.660913  [115264/464809]\n",
      "loss: 0.681485  [121664/464809]\n",
      "loss: 0.680655  [128064/464809]\n",
      "loss: 0.752008  [134464/464809]\n",
      "loss: 0.548631  [140864/464809]\n",
      "loss: 0.702283  [147264/464809]\n",
      "loss: 0.564092  [153664/464809]\n",
      "loss: 0.644610  [160064/464809]\n",
      "loss: 0.723615  [166464/464809]\n",
      "loss: 0.695758  [172864/464809]\n",
      "loss: 0.490325  [179264/464809]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[210], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# test_loop(val_dl, model, loss_fn)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[208], line 12\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\aujo8\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aujo8\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aujo8\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dl, model, loss_fn, optimizer)\n",
    "    # test_loop(val_dl, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
